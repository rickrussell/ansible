#!/bin/bash
#
# Script adapted from: https://gist.github.com/joemiller/6049831
#
# This script will attempt to detect any ephemeral drives on an EC2 node and create a RAID-0 stripe
# mounted at /mnt. It should be run early on first boot and can be configured with updstart.  If you reboot, raid
# set should persist.  If you shut down the instance, a new raid set will be created.  If you need the raid set to
# persist, then you should be using AWS EFS.
#
# REMINDER: The instance needs to be an m3.xlarge or similar which should include two extra ephemeral volumes.

# Set metadata url
METADATA_URL_BASE="http://169.254.169.254/2012-01-12"

detect_drive_scheme() {
  # Configure Raid - take into account xvdb or sdb
  root_drive=`df -h | grep -v grep | awk 'NR==4{print $1}'`
  if [ "$root_drive" == "/dev/xvda1" ]; then
    echo "Detected 'xvd' drive naming scheme (root: $root_drive)"
    DRIVE_SCHEME='xvd'
  else
    echo "Detected 'sd' drive naming scheme (root: $root_drive)"
    DRIVE_SCHEME='sd'
  fi
  # figure out how many ephemerals we have by querying the metadata API, and then:
  #  - convert the drive name returned from the API to the hosts DRIVE_SCHEME, if necessary
  #  - verify a matching device is available in /dev/
  drives=""
  ephemeral_count=0
  ephemerals=$(curl --silent $METADATA_URL_BASE/meta-data/block-device-mapping/ | grep ephemeral)
  for e in $ephemerals; do
    echo "Probing $e .."
    device_name=$(curl --silent $METADATA_URL_BASE/meta-data/block-device-mapping/$e)
    # might have to convert 'sdb' -> 'xvdb'
    device_name=$(echo $device_name | sed "s/sd/$DRIVE_SCHEME/")
    device_path="/dev/$device_name"
    # test that the device actually exists since you can request more ephemeral drives than are available
    # for an instance type and the meta-data API will happily tell you it exists when it really does not.
    if [ -b $device_path ]; then
      echo "Detected ephemeral disk: $device_path"
      drives="$drives $device_path"
      ephemeral_count=$((ephemeral_count + 1 ))
    else
      echo "Ephemeral disk $e, $device_path is not present. skipping"
    fi
  done

  if [ "$ephemeral_count" = 0 ]; then
    echo "No ephemeral disk detected. exiting"
    exit 0
  fi
}

unmount_mnt() {
  # ephemeral0 is typically mounted for us already. umount it here
  umount /mnt
}

zero_drives() {
  # overwrite first few blocks in case there is a filesystem, otherwise mdadm will prompt for input
  for drive in $drives; do
    dd if=/dev/zero of=$drive bs=4096 count=1024
  done
}

create_raid() {
  partprobe
  mdadm --create --verbose /dev/md0 --level=0 -c256 --raid-devices=$ephemeral_count $drives
  echo DEVICE $drives | tee /etc/mdadm.conf
  mdadm --detail --scan | tee -a /etc/mdadm.conf
  blockdev --setra 65536 /dev/md0
  mkfs -t ext4 /dev/md0
  mount -t ext4 -o noatime,nobootwait /dev/md0 /mnt
}

clean_fstab() {
  # Remove xvdb/sdb from fstab
  chmod 777 /etc/fstab
  sed -i "/${DRIVE_SCHEME}b/d" /etc/fstab
}

populate_fstab() {
  # Add mount info to fstab
  echo "/dev/md0 /mnt ext4 noatime,nobootwait 0 0" | tee -a /etc/fstab
}

add_mdadm_conf() {
  # Clean out mdadm.conf if it exits
  cp /etc/mdadm/mdadm.conf /root/mdadm.conf.old
  rm /etc/mdadm/mdadm.conf || true
  # Save our configuration in mdadm.conf in case of reboot
  mdadm --detail --scan >> /etc/mdadm/mdadm.conf
  update-initramfs -u
}

# Die if raid is configured. (Reboots)
# NOTE: If instance is stopped and started, ephemeral volumes are lost
# Raid volume will be recreated after instance is started.
if grep -Fq "md0" /proc/mdstat;
then
  echo "Raid already configured" && exit
else
  echo "Detecting drive schema. (sdb vs xvdb)"
  detect_drive_scheme
  echo "Unmounting current /mnt"
  unmount_mnt
  echo "Zeroing ephemeral disks"
  zero_drives
  echo "Creating Raid Volumes"
  create_raid
  echo "Cleaning and populating fstab"
  clean_fstab
  populate_fstab
  add_mdadm_conf
  echo "Done"
fi
